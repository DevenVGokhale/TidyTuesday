---
title: Predicting Concrete Compressive Strength
author: Deven Gokhale
format: 
    html:
        theme: slate
        code-fold: true
jupyter: python3
---

# Introduction:
To be filled in later 

```{python setup}

# packages 
import os 

root = "/Users/devengokhale/Documents/GitHub/TidyTuesday"
if os.getcwd() == root:
    os.chdir("./scripts/July10_2025/")

from typing import Dict

import polars as pl

import plotnine as p9
import seaborn as sns
import matplotlib.pyplot as plt 

import pymc as pm
import arviz as az
import pymc_bart as pmb
from sklearn.model_selection import train_test_split

import utility_fns as uf

```

```{python}
RANDOM_SEED = 9867478881
# set seaborn stye
sns.set_theme(style="whitegrid")
plt.style.use("dark_background")

# setting the data path relative to the working directory 
data_path = "data/"

```
```{python}
#\ label: Load data 
concrete = pl.read_excel(os.path.join(data_path, "Concrete_Data.xls"), sheet_name="Sheet1") 

new_column_names = [
    "cement", "blast_furnace_slag", "fly_ash", "water", 
    "superplasticizer", "coarse_aggregate", "fine_aggregate", 
    "age_days", "compressive_strength"
    ]

concrete.columns = new_column_names

#concrete_dict = uf.generate_split(concrete)

#concrete = concrete_dict["combined"]

concrete.head()

```

# Exploratory Data Analysis 
To be filled later ...

## Starting with visualizing some relationships 

### Describing the the distribution of data 
These are raw values 
```{python}
#| label: boxen_plot
#| echo: false
#| fig-cap: "This is figure caption"
plt.figure()
sns.boxenplot(concrete.unpivot(), x = "value", y="variable")
```

Standardizing the dataset for a better comparsion across all the values -- 

```{python}
concrete = (
    concrete
    .with_columns(
        (pl.col("age_days").log()).alias("log_age_days")
        )
)

concrete_std = (
    concrete
    .select([
        ((pl.col(c) - pl.col(c).mean())/pl.col(c).std()).alias(c)
        for c in concrete.columns
    ])
)

concrete_std = concrete_std.drop("age_days")

concrete_std.head()
```

replotting the distribution of the standardized data 

```{python}
#| label: boxen_plot_std
plt.figure()
sns.boxenplot(concrete_std.unpivot(), x = "value", y="variable")
```

### Pairwise corelations plot 
```{python}
#| label: pairwise_corelation 
#| echo: false
#| fig-cap: "This is a figure caption"
plt.figure()
sns.pairplot(concrete_std.to_pandas(), diag_kind="kde")
```

### Clustering between the variables to check the 
```{python}
plt.figure()
sns.clustermap(concrete_std.to_pandas().corr(), cmap="coolwarm", annot=True, vmin=-1, vmax=1)
``` 

# Modeling using PyMC
Trying out pymc-bart  -- finally 

more text to come 

```{python}

splits = uf.generate_split(concrete_std)
# unpack the values to 
X_train, X_test, y_train, y_test = splits["train"][0], splits["test"][0], splits["train"][1], splits["test"][1]

X_cols = X_train.columns
y_col = y_train.columns

X_train = X_train.drop("partition").to_pandas()
y_train = y_train.to_pandas().squeeze()
X_test = X_test.drop("partition").to_pandas()
y_test = y_test.to_pandas().squeeze()

combined = splits["combined"]
```


```{python}

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(type(y_train))


```

Quickly checking distributions before moving to model the response using BART -- 

```{python}

#\ label: standardaized_distribution 
#| echo: fakse
#| fig-cap: 
plt.figure()
sns.boxenplot(combined.unpivot(index="partition"), x="value", y="variable", hue="partition")
```


similarly hoping that scatter plots also look pretty similar

```{python}
#| label: pairwise_corelation_split 
#| echo: false
#| fig-cap: "This is a figure caption"
plt.figure()
sns.pairplot(
    combined.to_pandas(), diag_kind="kde", 
    hue="partition", plot_kws= {"alpha":0.2,}
    )
```


```{python}
#\ label: Fitting full model
#| cache: true 
with pm.Model() as model_compress:

    X = pm.Data("X", X_train)
    y = pm.Data("y", y_train)
    
    σ = pm.HalfNormal("σ", 1)
    μ = pmb.BART("μ", X, y)
    
    y_hat = pm.Normal("y_hat", mu=μ, sigma=σ, observed=y)

    idata_compress = pm.sample(tune=1000)
    
    y_sample_train = pm.sample_posterior_predictive(idata_compress, 
                                                    extend_inferencedata=True,
                                                    random_seed=RANDOM_SEED)
```

```{python}
idata_compress
```
```{python}
az.plot_trace(idata_compress, var_names="σ")
```

```{python}
pmb.plot_convergence(idata_compress, var_name="μ")
```


```{python}
# out of sample --- sample from the posterior to then compare with the oos test set
with model_compress:
    X.set_value(X_test)
    y.set_value(y_test)
    y_sample_test = pm.sample_posterior_predictive(idata_compress, random_seed=RANDOM_SEED)

```


```{python}
fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,7), 
                       sharex=True, sharey=True, layout="constrained")
az.plot_ppc(y_sample_train, kind="cumulative", observed_rug=True, ax=ax[0])
ax[0].set(title="Posterior Predictive Check (train)", ylabel="Compression Strength", xlabel="")
az.plot_ppc(y_sample_test, kind="cumulative", observed_rug=True, ax=ax[1])
ax[1].set(title="Posterior Predictive Check (test)", ylabel="Compression Strength")

# Collect legend handles from one axis (both have same labels)
handles, labels = ax[0].get_legend_handles_labels()

# Add shared legend to the figure
fig.legend(handles, labels, loc='upper center', ncol=len(labels), bbox_to_anchor=(0.5, 1.02))
```


```{python}
# resetting the values of X an Y to train to run a few other analyses
with model_compress:
    X.set_value(X_train)
    y.set_value(y_train)

# Define layout
fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 6))
axes = axes.flatten()  # flatten to 1D list
# Plot PDP with custom axes
pmb.plot_pdp(
    μ,               
    X_train,
    ax=axes          
)
plt.tight_layout()
```

```{python}
vi_results = pmb.compute_variable_importance(idata_compress, μ, X_train)

print(X_train.columns[vi_results["indices"]])

labels = [
    "Log age\n(days)",
    "Cement",
    "Blast furnace\nslag",
    "Water",
    "Superplasticizer",
    "Fine\naggregate",
    "Fly ash",
    "Coarse\naggregate"
]

pmb.plot_variable_importance(vi_results, labels=labels, plot_kwargs={"color_r2": "white"})
```


```{python}
az.plot_energy(idata_compress)
```

```{python}
# getting rid of the nuissance variables 
X_train_reduced = (
    pl.DataFrame(X_train)
    .select("log_age_days", "cement", "blast_furnace_slag", "water")
)

X_test_reduced = (
    pl.DataFrame(X_train)
    .select("log_age_days", "cement", "blast_furnace_slag", "water")
)
```

```{python}
# curious if function will play nicely 
def fit_model(X: pl.DataFrame, y:pl.Series): 
    # transform to usable datatypes
    X_ = X.to_pandas()
    # fit the model 
    with pm.Model() as model:

        X = pm.Data("X", X)
        y = pm.Data("y", y)
    
        σ = pm.HalfNormal("σ", 1)
        μ = pmb.BART("μ", X, y)
    
        y_hat = pm.Normal("y_hat", mu=μ, sigma=σ, observed=y)

        idata = pm.sample(tune=1000)
        
        y_post_pred = pm.sample_posterior_predictive(idata_compress, 
                                                     extend_inferencedata=True,
                                                     random_seed=RANDOM_SEED
                                                     )
        return idata_compress, y_post_pred, model

# refit the model 
reduced_model = fit_model(X_train_reduced, y_train)
```

```{python}
az.plot_ppc(reduced_model[1], kind="cumulative", observed_rug=True)
```
```{python}

```